# INRIA BCI Challenge
This repository contains the code I used for the winning solution for the INRIA BCI challenge hosted on Kaggle.

https://www.kaggle.com/c/inria-bci-challenge

# Hardware and OS
* 12 Intel Celeron M processors (1.5GHz), 64 GB RAM, 512 GB SSD 
* Ubuntu 14.04.2

# Software and version numbers
To fit and predict from the model
* Python - 2.7.6
  * scikit_learn - 0.14.1
  * numpy - 1.8.2
  * pandas - 0.13.1

To generate the features
* Matlab - R2014b (see www.mathworks.com)
  * JSONlab (this can be downloaded from http://www.mathworks.com/matlabcentral/fileexchange/33381-jsonlab--a-toolbox-to-encode-decode-json-files-in-matlab-octave)

n.b. it may be possible to use Octave (free) instead of Matlab (proprietary) but I haven't tested my code using Octave. 

# Settings.json
```
{
  "data": "data_dir",
  "features": "features_dir",
  "trained_model": "/trained_model_dir",
  "submission": "submissions_dir"
}
```
* `data_dir` - directory containing the downloaded competition data
* `features_dir` - directory containing the features extrated from the raw data
* `trained_model_dir` - directory containing the trained model
* `submissions_dir` - directory submissions are written to
# Generating features
There are five Matlab scripts (`generate_meta_features.m`, `get_sess5_retrial_features.m`, `get_ave_amplitude_features.m`, `get_template_features1.m` and `get_template_features1.m`) which will generate the features used in my model. It's important to run these scripts in order as latter scripts use data produced by the former scripts. First run  `generate_meta_features.m`, then `get_sess5_retrial_features.m`, next `get_ave_amplitude_features.m`, then `get_template_features1.m` and finally `get_template_features1.m`.

To run `generate_meta_features.m`, simply type in the following from within matlab 

`generate_meta_features`

or from within a terminal

`matlab -r "generate_meta_features"`

You will see the following printed to screen

```
Obtaining meta features for training set .....
Training set subject 2, session 1
Training set subject 2, session 2
```

After this script is complete, run the other scripts.

On my machine `generate_meta_features.m` takes ~5 mins to run, `get_sess5_retrial_features.m` ~1 min,  `get_ave_amplitude_features.m` a few days, `get_template_features1.m` a few days and `get_template_features1.m` ~1 day. There is a lot of scope to parallelise the code in `get_ave_amplitude_features.m`, `get_template_features1.m` and `get_template_features1.m` to speed things up.

All feature files are saved in the `features_dir` directory. For convenice I've placed the feature files generated by these scripts in the `feature_files` folder within this repository, so there is no need to generate them from scratch.

# Description of features
* `generate_meta_features.m` - Generates 'meta' features including trial timestamp, session number etc
* `get_sess5_retrial_features.m` - Attempts to infer the class labels for session 5 based on the time between feedback events  
* `get_ave_amplitude_features.m` - Mean EEG values in windows of various lengths and lags after the feedback
* `get_template_features1.m` - All ERP signals labelled as positive feedback in the training set are averaged to create a 'positive feedback template' and similarly to create a 'negative feedback template'. Correlation, covariance, and euclidean distance between the templates and test ERP signals as used as the basis for features. 
* `get_template_features2.m` - As above except the features are based on the cross-correlation and cross-covariance.

# Fitting the model
As my final model is a weighted average of two models which use different feature sets, my code generates two models. To fit the models run the following in a terminal

`python train_model.py`

The following will appear on the screen
```
Model 1 (SVM, linear kernel, meta and average amplitude features), loading data ....
Model 1, applying subset selection ....
Model 1, training ....
Model 1, saving files ....
Model 2 (SVM, linear kernel, meta and template features), loading data ....
Model 2, training ....
Model 2, saving files ....
```

This takes ~ 15 mins to run on my machine. The two models, together with an array containing index values (used for the sub-set selection), will be saved to the `trained_model_dir` directory. For convenienve, all these files can be found in the `trained_model` folder within this repository.

# Using the model to predict
Run `predict.py`

`python predict.py`

You will see the following

The predictions will be saved to the `submissions_dir` directory. I've put a copy of the predictions file in the `submissions` directory within this repository. 
